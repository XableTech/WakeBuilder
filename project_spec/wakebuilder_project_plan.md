# WakeBuilder - Implementation Plan

## Overview

This implementation plan guides you through building WakeBuilder, the training platform that enables users to create custom wake word detection models. The plan is organized into phases that build upon each other, starting with the foundational components and progressing toward the complete dockerized system with web interface. Each phase includes detailed explanations of what you're building and why it matters, helping you understand the system deeply as you construct it.

## Phase One: Establishing the Foundation

The first phase focuses on setting up the core technology that makes wake word training possible. Think of this as building the engine that powers everything else. Before you can train custom wake words, you need a working base speech model and audio processing pipeline that can convert sound into the numerical representations that neural networks understand.

Your first major task is selecting and preparing the base speech embedding model. This model is the secret ingredient that allows your system to train on one person's voice but work for everyone. You'll want to search TensorFlow Hub or Hugging Face for pre-trained models designed for audio classification, specifically looking for ones trained on large speech datasets like AudioSet or Google Speech Commands. These datasets contain hundreds of thousands of audio samples from diverse speakers across many accents and languages, which means the model has already learned what human speech sounds like in all its variations. When you find a suitable model, download it and carefully examine its architecture to understand what input format it expects and what output embeddings it produces. The embeddings are compact numerical representations, typically vectors of a few hundred numbers, that capture the essential acoustic characteristics of the audio while discarding irrelevant details. You'll then convert this model from its original format into ONNX, which is an open standard for neural networks that runs efficiently on CPU and works across different platforms. The conversion process typically involves loading the model in its native framework, running a test input through it to verify it works, and then exporting to ONNX while ensuring the input and output tensor shapes match what you expect.

The second task is building your audio preprocessing pipeline, which transforms raw audio waveforms into mel spectrograms that the base model can process. Raw audio is just a sequence of numbers representing air pressure at different moments in time, which isn't a convenient representation for neural networks to work with. Mel spectrograms convert this time-domain signal into a time-frequency representation that looks like a heatmap showing how different frequencies evolve over time. The mel scale is designed to match how humans perceive pitch, so frequencies are spaced in a way that emphasizes the ranges most important for understanding speech. You'll use the librosa library to compute these spectrograms, and it's crucial that you use exactly the same parameters that the base model was trained with. This typically means a sample rate of sixteen kilohertz, specific window sizes for the Fourier transform that balance time resolution with frequency resolution, and a particular number of mel frequency bins. If your preprocessing doesn't match what the model expects, the embeddings will be meaningless. Create a standalone function that takes raw audio as input and produces a properly formatted spectrogram, then test it extensively with various audio samples to ensure it produces sensible-looking results.

The third task is organizing your development environment and project structure. You'll need to install PyTorch for training neural networks, ONNX Runtime for running models efficiently, and librosa for audio processing. Set up a clear directory structure that separates different concerns. Create a folder for the training pipeline code where the model training logic will live, another for the FastAPI backend that will serve the web interface, and a third for the frontend web application files. This separation makes the codebase easier to navigate and maintain as it grows. At this stage, create a simple test script that validates your foundation works correctly. This script should load the base speech model in ONNX format, read a short audio file like someone saying a word, convert the audio to a mel spectrogram using your preprocessing pipeline, run the spectrogram through the base model to get embeddings, and print out the embedding vector. If you see a vector of numbers in a reasonable range without any errors or warnings, your foundation is solid and you're ready to build the training system on top of it.

## Phase Two: Building the Training Pipeline

With your foundation in place, phase two focuses on creating the machinery that actually trains wake word models. This is where the magic happens, where a handful of voice recordings transform into a model that can detect that specific wake word reliably. Understanding this phase deeply is crucial because the quality of your training pipeline directly determines the quality of the wake word models your system produces.

The first component you'll build is the data augmentation system, which is how your system creates hundreds of training examples from just three to five recordings. Start by integrating Piper TTS into your project. Download and set up Piper, then acquire multiple voice models that represent diversity in gender, age, and accent. You'll want at least five to ten different voices, and more is better for producing robust models. Create a function that accepts a text string like "Phoenix" and a list of voice model paths, then generates synthetic audio of each voice speaking that text. But don't stop at just generating the text once per voice. Vary the speaking rate by generating versions at eighty percent normal speed, normal speed, and one hundred twenty percent speed. Vary the pitch by shifting it up and down by a couple of semitones. These variations simulate how different people naturally speak the same words with slightly different cadences and tones. Next, implement audio mixing functions that add background noise to your clean recordings. Collect or generate noise samples representing common environments like office chatter, traffic sounds, rain, music, and household appliances. Mix these noises with your wake word audio at various signal-to-noise ratios, perhaps ranging from negative five decibels for very noisy conditions up to negative twenty decibels for quieter environments. Also implement volume randomization that scales the overall loudness up and down within a reasonable range. The combination of user recordings, synthetic variations, speed adjustments, pitch shifts, background noise, and volume changes should give you five hundred to one thousand diverse training examples from your starting set of three to five recordings.

The second component is the negative example generator, which teaches your model what not to trigger on. This is just as important as the positive examples because without good negatives, your model will trigger constantly on random speech or sounds. Implement two strategies for generating negatives. First, use Piper TTS to generate speech that doesn't contain the wake word. If the wake word is "Phoenix," generate phonetically similar words like "finish," "fee knicks," or "free mix" that might confuse a poorly trained model. Also generate completely random sentences with common words to represent general speech. The phonetically similar negatives help your model learn fine-grained distinctions, while the random speech negatives teach it to ignore ordinary conversation. Second, include non-speech audio like pure silence, various noise types, and recordings of environmental sounds. Collect or generate samples of white noise, pink noise, office sounds, street sounds, and household sounds. These non-speech negatives ensure the model doesn't trigger on purely noise-based signals. Balance your dataset so that you have about three to five negative examples for every positive example. This ratio is important because wake words are rare events in continuous audio streams, and you want your model to reflect that rarity by being conservative about triggering.

The third component is the training loop that creates the wake word classifier. This classifier is a small neural network that you'll design with just two or three fully connected layers. The input to this network is the embedding vector from the base model, which might be something like five hundred twelve numbers. The hidden layers should be smaller, perhaps two hundred fifty six and one hundred twenty eight neurons, with ReLU activation functions between them. The output is a single neuron with a sigmoid activation that produces a probability between zero and one indicating wake word presence. Implement the training process by first processing all your augmented audio samples through the base model to extract embeddings. Store these embeddings along with their labels, which are one for positive examples containing the wake word and zero for negative examples. Split your dataset into training and validation sets, holding out about twenty percent of the data for validation. This split is crucial for detecting overfitting, where the model memorizes training examples but doesn't generalize to new audio. Create the training loop using PyTorch, where each iteration randomly samples a batch of examples, feeds them through the classifier, computes the binary cross-entropy loss comparing predictions to true labels, backpropagates gradients, and updates the classifier weights using the Adam optimizer. Start with a learning rate around point zero zero one and train for perhaps twenty to fifty epochs, which means complete passes through the training data. Monitor both training loss and validation loss after each epoch. If training loss decreases but validation loss increases, that's overfitting and you should stop training. Include early stopping logic that halts training when validation performance stops improving for several consecutive epochs.

The fourth component is model evaluation and threshold calibration, which determines the optimal sensitivity setting for your trained model. After training completes, you need to systematically test the model on the validation set to understand its behavior at different confidence thresholds. Create a function that runs the validation examples through your trained model and records the confidence score for each example. For the positive examples, these confidence scores tell you how strongly the model believes the wake word is present. For the negative examples, the scores tell you how often the model incorrectly thinks it hears the wake word. Now compute performance metrics at different threshold values ranging from zero point three to zero point nine in increments of zero point one. For each threshold, count how many positive examples score above the threshold, which gives you the true positive rate, and how many negative examples score above the threshold, which gives you the false positive rate. The true positive rate tells you what percentage of real wake words will be detected, while the false positive rate tells you how often you'll get spurious triggers. Plot these metrics to visualize the trade-off. A good default threshold is typically where these two rates are roughly equal, creating a balanced detector. However, some applications might prefer higher sensitivity at the cost of more false triggers, or vice versa. Save this recommended threshold in your model metadata file so that WakeEngine knows what default sensitivity to use. Also compute and save the validation accuracy and the area under the ROC curve, which are standard metrics that help users understand model quality.

## Phase Three: Creating the FastAPI Backend

Phase three builds the web server that orchestrates the training process and communicates with the web interface. This backend is the bridge between the user-facing interface and the training machinery you built in phase two. Understanding how to structure a backend for long-running processes is key to creating a responsive user experience.

Start by designing your API endpoint structure, thinking carefully about the workflow users will follow. You need an endpoint for initiating training that accepts a POST request with the wake word text and audio recordings encoded as base64 or multipart form data. This endpoint should validate the inputs, ensuring the wake word text is reasonable and the audio recordings are in a format you can process. Then immediately return a job identifier and a success message, without waiting for training to complete. This asynchronous pattern is crucial because training takes minutes and you can't leave HTTP connections hanging that long. Create another endpoint for checking job status that accepts a job identifier and returns detailed information about training progress. This endpoint should return a JSON object with fields for the current phase like data augmentation or neural network training, the percentage complete, and any status messages that help users understand what's happening. Build an endpoint for downloading completed models that streams the ONNX file as binary data with appropriate content-type headers. Add an endpoint for listing all available models that returns JSON describing both default models and any custom models the user has created, including metadata like the wake word text and creation timestamp. Include an endpoint for deleting custom models that removes both the model file and its metadata. Finally, create a WebSocket endpoint for real-time testing that accepts streaming audio from the browser and returns detection events as they occur.

The job management system requires careful design because you're running long-running tasks in the background while serving web requests in the foreground. You have several options for implementing this, ranging from simple to sophisticated. The simplest approach uses Python's built-in threading module to spawn a new thread for each training job. Create a dictionary that maps job identifiers to job status objects, where each status object contains fields for the current phase, progress percentage, and any error messages. When training starts, create an entry in this dictionary and launch a thread that runs the training function. The training function should update the status object as it progresses through data augmentation, negative generation, neural network training, and evaluation. The status endpoint simply looks up the job identifier in the dictionary and returns the current status. This approach works well for a single-user system but doesn't scale to high concurrency. For a more robust solution, consider using Celery with Redis as a message broker, which provides task queuing, failure handling, and distributed execution. However, given that WakeBuilder is designed for local use, the simpler threading approach is probably sufficient and avoids additional dependencies.

File storage and organization needs thoughtful implementation to maintain user privacy and system cleanliness. When audio recordings arrive from the frontend, save them to a temporary directory with a unique identifier associated with the training job. Use a library like uuid to generate these identifiers to avoid collisions. During training, read the audio files from this temporary location. Once training completes successfully or fails, delete the temporary audio files to respect user privacy and conserve disk space. When training succeeds, save the resulting model files to a permanent models directory organized by wake word name and timestamp. Create a consistent naming convention like "phoenix_2024_01_15_14_30_45.onnx" that makes files self-documenting. Save a JSON metadata file alongside each model with fields for the wake word text, creation timestamp, recommended threshold, and validation metrics. Implement proper error handling throughout the file operations, ensuring that missing directories are created, file write failures are caught and reported, and partial files from failed training runs are cleaned up properly.

The real-time testing endpoint is particularly interesting because it requires WebSocket communication for streaming audio. Implement a WebSocket handler in FastAPI that accepts connections with query parameters specifying which model to test and what sensitivity threshold to use. When a client connects, load the specified model into memory. Then enter a loop that receives audio chunks from the client, processes them through your wake word detection pipeline using the same process method that WakeEngine will implement, and sends back detection events as JSON messages. The detection events should include a timestamp, whether a detection occurred, and the confidence score. This gives users immediate feedback about their model's behavior. Handle client disconnections gracefully by cleaning up loaded models and breaking out of the receive loop. Implement error handling for malformed audio data or invalid model identifiers, sending error messages back to the client over the WebSocket connection so they understand what went wrong.

## Phase Four: Developing the Web User Interface

Phase four creates the user-facing web application that makes your sophisticated training system accessible to non-technical users. A well-designed interface can make the difference between a powerful system that nobody can use and one that empowers everyone. Think carefully about the user journey and how to guide people through the process smoothly.

Begin with the home page, which should immediately communicate what WakeBuilder does and what users can accomplish with it. Write a brief, friendly introduction explaining that this system lets them create custom wake words for voice-activated applications. Display a grid or list of available models prominently, showing both the default models that ship with the system and any custom models the user has created. Each model should be presented as a card with the wake word text as a prominent headline, the type indicated as either default or custom, and the creation date shown for custom models. Include action buttons on each card for testing the model, downloading it for use with WakeEngine, and deleting it if it's a custom model. Arrange these elements so the page feels organized and easy to scan. Add a prominent call-to-action button labeled something like "Create New Wake Word" that takes users to the training wizard. Use visual hierarchy through font sizes, colors, and spacing to guide attention to the most important elements.

The training wizard is where users create their custom wake words, and it should feel like a guided journey with clear progress. Implement this as a multi-step interface where each step focuses on one task and provides all the guidance needed to complete it successfully. The first step is wake word input, where users type their desired phrase. Show a text input field with placeholder text like "Enter your wake word" and real-time validation feedback. As they type, check that the input contains only letters and spaces, is between one and twenty characters, and consists of one or at most two words. Display validation messages in a friendly tone, like "Please use only letters and spaces" rather than technical error codes. When the input is valid, enable a next button that advances to the recording step. The second step is voice recording, where the interface needs to make the process feel easy and give users confidence that they're doing it right. Request microphone permission using the browser's WebRTC API with a clear explanation of why you need it. Display a large record button that changes appearance when recording is active, perhaps turning red or showing an animated pulsing effect. Show a waveform visualization or level meter so users can see that their voice is being captured. After each recording, play it back automatically so users can hear what was captured, and provide a re-record button if they're not satisfied. Collect three to five recordings total, making it clear how many more are needed. Once sufficient recordings are collected, enable a start training button that submits everything to the backend.

The third step shows training progress, transforming what could be an anxious waiting period into an informative experience. After submitting the training job, display an animated progress indicator that updates based on status information from your backend API. Use descriptive phase names rather than technical ones, like "Creating voice variations" instead of "Data augmentation" or "Teaching the model" instead of "Neural network training." Show percentage progress within each phase if possible. Include estimated time remaining based on typical training durations, perhaps saying something like "This usually takes five to ten minutes." The interface should poll the status endpoint regularly, perhaps every second, to get updates. When training completes successfully, automatically transition to the testing step. If training fails, show a clear error message explaining what went wrong and offering to restart the process. The fourth step is immediate testing, where users validate that their new model works. This page should feel exciting and rewarding because they've just created something custom. Display a simple visual indicator like a circle that pulses or changes color when the wake word is detected. Start listening automatically when the page loads so users can immediately try saying their wake word. Show a log of detections with timestamps so they can see the history. Include a sensitivity slider that lets them adjust the threshold in real time, helping them understand the trade-off between catching every utterance and avoiding false triggers. Provide clear next steps like downloading the model or returning to the home page to create another wake word.

For visual design, prioritize clarity and simplicity over elaborate graphics. Use a clean layout with plenty of whitespace so elements don't feel cramped. Choose a color scheme that's pleasant but not distracting, perhaps using a calming blue or green for primary actions. Make sure text is large enough to read comfortably and has sufficient contrast with backgrounds. Use plain language throughout, avoiding jargon and explaining concepts in terms anyone can understand. For example, instead of "model inference threshold," say "how easily the wake word triggers." Add tooltips or help icons next to potentially confusing elements that provide additional explanation when clicked or hovered. Test your interface on different screen sizes to ensure it works well on both desktop computers and tablets. Pay attention to loading states, showing spinners or skeleton screens while data is being fetched so users never wonder if the application is frozen.

## Phase Five: Dockerization and Deployment Preparation

Phase five packages your entire training platform into a self-contained Docker image that users can run with a single command. Docker solves the notorious problem of "works on my machine" by bundling your application with all its dependencies in a consistent environment. Understanding Docker basics and how to create efficient images is essential for distributing WakeBuilder widely.

Start by writing a Dockerfile that defines how to build your container image. Begin from an official Python base image, probably python:3.9-slim, which provides Python without unnecessary bloat. The first section of your Dockerfile should install system-level dependencies that your Python packages need. Audio processing requires libraries like libsndfile1 for reading audio files and ffmpeg for audio format conversion. Use the apt-get package manager on Debian-based images to install these, and clean up the apt cache afterward to keep the image size reasonable. Next, create a working directory inside the container, perhaps /app, where your application code will live. Copy your requirements.txt file into this directory and run pip install to install all Python dependencies. This two-step process of copying requirements first and then copying code leverages Docker's layer caching, meaning dependency installation only reruns when requirements change, not every time you modify code.

The next section of your Dockerfile should download and integrate Piper TTS voice models. Create a directory like /app/tts_voices and use wget or curl to download voice model files from Piper's distribution locations. Include multiple voices representing different genders and accents to ensure diverse training data. You'll also want to copy your pre-trained default wake word models into the image so they're immediately available when users start the container. Create a directory like /app/default_models and copy your ONNX files and metadata JSON files there. Document in comments which default wake words are included and where they came from. Copy your application code including the FastAPI backend, training pipeline, and web frontend files into the container's working directory. Set up any environment variables your application needs, such as the path to model directories or logging configuration. Expose port 8000, which is the standard port for your FastAPI application. Finally, set the container's entry point to launch FastAPI using uvicorn, perhaps with a command like "uvicorn backend.main:app --host 0.0.0.0 --port 8000" that makes the server accessible from outside the container.

Create a docker-compose.yml file that makes running WakeBuilder even simpler. Docker Compose lets you define your container configuration declaratively, including port mappings and volume mounts. The compose file should define a service for WakeBuilder that builds from your Dockerfile, maps port 8000 inside the container to port 8000 on the host machine, and mounts a volume for persistent model storage. The volume mount is crucial because it ensures that custom wake words users create survive container restarts and updates. Map a directory like ./models on the host machine to /app/models inside the container. This way, when users create custom models, the files are written to their local disk rather than being trapped inside the container. Include environment variables in the compose file if needed for configuration. With this setup, users can launch WakeBuilder by simply running "docker-compose up" in the directory containing your compose file, and everything starts automatically.

Write comprehensive documentation explaining how to use the Docker deployment. Start with prerequisites, explaining that users need Docker and Docker Compose installed on their system, with links to installation instructions for different operating systems. Provide a quick start section with the exact commands to run, something like "docker-compose up" followed by "Open your browser to http://localhost:8000". Explain the volume mount so users understand where their trained models are stored and how to back them up. Document system requirements based on your testing, specifying that eight gigabytes of RAM is recommended and that training will be faster on multi-core processors. Include troubleshooting information for common issues like port conflicts if another application is using port 8000, or permission errors if the volume mount directory isn't accessible. Consider providing pre-built images on Docker Hub so users don't even need to build the image themselves, they can just pull and run.

## Phase Six: Testing, Optimization, and Polish

The final phase focuses on ensuring WakeBuilder works reliably and performs well across different scenarios. This testing and optimization phase is where you transform a working prototype into a polished system that people will trust and enjoy using. Thorough testing finds problems before users do, and optimization ensures the system runs efficiently on modest hardware.

Begin with comprehensive functional testing across many different wake words. Test single-word wake words like "computer," "assistant," and "listen." Test two-word phrases like "hey there" and "wake up." Try wake words with unusual phonetics or sounds borrowed from other languages. Test very short words like "go" or "hi" and longer phrases approaching your maximum length. For each wake word, run through the complete training process and evaluate the resulting model quality. Record metrics like how often the model correctly detects the wake word when you say it and how often it falsely triggers on other speech or sounds. If you find that certain types of wake words consistently produce poor models, investigate why. Perhaps your data augmentation doesn't generate enough variations for short words, or your negative examples don't include enough phonetically similar words. Adjust your training pipeline based on what you learn and retest.

Performance optimization is critical for a CPU-based system. Use Python profiling tools like cProfile to identify bottlenecks in your training pipeline. Run a complete training job under the profiler and examine where time is being spent. The most likely hot spots are mel spectrogram computation, neural network inference for generating embeddings, and the training loop itself. For spectrogram computation, ensure you're using librosa's optimized functions and not doing redundant work. Consider caching spectrograms if you process the same audio multiple times. For base model inference, experiment with ONNX Runtime's execution providers and settings. The CPU execution provider has options for parallelization and optimization level that can significantly impact speed. Try different configurations and benchmark them. For the training loop, ensure you're using appropriate batch sizes that keep both CPU and memory utilized efficiently. Consider using PyTorch's DataLoader with multiple worker processes to parallelize data loading. Look into model quantization, which converts neural network weights from 32-bit floats to 8-bit integers. This can substantially speed up inference with minimal accuracy loss, though it requires careful validation to ensure the quantized model performs acceptably.

Test your Docker container on different operating systems to catch platform-specific issues. Try Linux, macOS, and Windows with Docker Desktop. Verify that the container builds successfully on each platform and that the web interface is accessible. Test audio capture in the browser on each operating system to ensure WebRTC works correctly. Check file paths and permissions, as these can vary between platforms. Verify that volume mounts work properly and that trained models persist correctly. Test with different Docker and Docker Compose versions if possible, as version incompatibilities can cause subtle issues. Document any platform-specific requirements or known issues in your README.

Create extensive user documentation that covers every aspect of using WakeBuilder. Write a quick start guide that takes someone from installing Docker to training their first wake word in under fifteen minutes, with step-by-step instructions and screenshots at each stage. Create a detailed user manual explaining each page of the web interface and what all the options mean. Include a section on how the technology works under the hood for curious users who want to understand what's happening. Be honest about limitations, such as the fact that wake words that sound similar to common speech patterns will have higher false trigger rates, or that training quality depends on recording clear, consistent voice samples. Write a troubleshooting guide addressing common problems like microphone not being detected, training jobs failing, or models not performing well. Include solutions or workarounds for each issue. Create a FAQ section answering questions you anticipate users will have about privacy, model quality, and system requirements.

The final step is preparing for release. Review all your code for quality, ensuring it follows consistent style and includes helpful comments explaining complex sections. Write or update docstrings for all functions and classes. Set up automated testing if you haven't already, creating a test suite that validates critical functionality. Create a GitHub repository or similar for your code with a clear README that explains what WakeBuilder is, how to use it, and how to contribute. Choose an appropriate open-source license, likely MIT or Apache 2.0, that allows others to use and modify your work freely. Write a changelog documenting what's included in your initial release. Consider creating a simple website or documentation site that showcases WakeBuilder and provides resources for users. With all this preparation complete, you'll have a professional, polished system that others can use confidently.